# -*- coding: utf-8 -*-
"""lstm_predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N_Ybi3LufSEdN6DRC7qgVQ6Zj815Ym0X
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy import stats
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.seasonal import seasonal_decompose
import tensorflow as tf

from module import periodic_check

# dataset = pd.read_csv('data/JNJ.csv', usecols=[1], engine='python', skipfooter=3)
# plt.plot(dataset)
# plt.show()

# # 畫出 ACF 12 期的效應
# sm.graphics.tsa.plot_acf(dataset, lags=12)
# plt.show()

# # 畫出 PACF 12 期的效應
# sm.graphics.tsa.plot_pacf(dataset, lags=12)
# plt.show()

import math
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

# 產生 (X, Y) 資料集, Y 是下一期的乘客數
def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return np.array(dataX), np.array(dataY)

def read_data(f):
	df = pd.read_csv(f)
	df[["hour","min","sec"]] = df["End Time"].str.split(':',expand=True)
	df["hour"] = (df["hour"].astype(int) - 1).astype(str).apply(lambda x: '{0:0>2}'.format(x))
	df["time"] = df["End Date"] + " " + df["hour"] + ":" +df["min"] + ":" + df["sec"]
	df = df.drop(columns=["End Date","End Time","hour","min","sec","Type","OrganisationURI","OrganisationLabel","Air Quality Monitoring Station","Units"])
	df["PM10"] = df["PM10"].fillna(value=0)
	df = df.set_index('time')
	df.index = pd.to_datetime(df.index,format="%d/%m/%Y %H:%M:%S")
	ts = df.loc['2014-01-01':'2014-03-01']
	ts = ts[ts["Location"] == "Finchley"]["PM10"]

	return ts

period = 168

# 載入訓練資料
dataframe = pd.read_csv('data/air-quality-monitoring-2014-1.csv', usecols=[7], engine='python')
dataframe["PM10"] = dataframe["PM10"].fillna(value=0)
dataset = dataframe.values
dataset = dataset.astype('float64')

# 正規化(normalize) 資料，使資料值介於[0, 1]
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataframe)

# 2/3 資料為訓練資料， 1/3 資料為測試資料
train_size = int(len(dataset) * 0.67)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]

# 產生 (X, Y) 資料集, Y 是下一期的乘客數(reshape into X=t and Y=t+1)
look_back = 1
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)
# reshape input to be [samples, time steps, features]
trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

# 建立及訓練 LSTM 模型
model = Sequential()
if os.path.isfile("my_model.h5"):
	model = tf.keras.models.load_model('my_model.h5')
else:
	model.add(LSTM(4, input_shape=(1, look_back)))
	model.add(Dense(1))
	model.compile(loss='mean_squared_error', optimizer='adam')
	model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)
	model.save('my_model.h5')

# 預測
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)

# 回復預測資料值為原始數據的規模
trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform([testY])

# calculate 均方根誤差(root mean squared error)
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))

# 畫訓練資料趨勢圖
# shift train predictions for plotting
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict

# STL
df = read_data('data/air-quality-monitoring-2014-1.csv')
ts = periodic_check.diff_smooth(df)
ts = ts.astype('float64')
decomposition = seasonal_decompose(ts, period=period, two_sided=False)
trend = decomposition.trend
residual = decomposition.resid

d = residual.describe()
delta = d['75%'] - d['25%'] * 0.5
low_error, high_error = (d['75%'] - 1 * delta, d['25%'] + 1 * delta)

# 畫測試資料趨勢圖
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict

low_conf = testPredictPlot * low_error
low_conf[low_conf < 0] = 0
high_conf = testPredictPlot * high_error
high_conf[high_conf < 0] = 0

# 最後一筆的乘客數當作X，預測下個月的乘客數
# last_row = scaler.transform(testY[0][-1].reshape((-1, 1)))
# last_row = np.reshape(last_row, (last_row.shape[0], 1, last_row.shape[1]))
# test2Y = scaler.inverse_transform(testPredict2)

# testY = np.append(testY,test2Y)

# 畫原始資料趨勢圖
# plot baseline and predictions

# plt.plot(scaler.inverse_transform(dataset))
plt.plot(trainPredictPlot,color='blue', label='Predict')
plt.plot(testPredictPlot,color='red', label='Original')
plt.plot(low_conf,color='grey', label='low')
plt.plot(high_conf,color='grey', label='high')

plt.legend(loc='best')
plt.show()